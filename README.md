# GCP Data Architecture

Documento que apresenta uma arquitetura de dados, baseada em GCP, para disponibilizar informa√ß√µes recebidas de diversas fontes de forma eficiente, tanto para consumo r√°pido quanto para uso anal√≠tico.

A solu√ß√£o foi pensada para operar em larga escala, com alto volume de dados, mas pode ser simplificada para cen√°rios com menor carga ou complexidade.

<p align="center">
  <img src="https://raw.githubusercontent.com/Bruno-Furtado/gcp-data-architecture/refs/heads/main/architecture.webp" alt="architecture">
</p>

## 1. Envio de dados üèåÔ∏è‚Äç‚ôÇÔ∏è

Tr√™s s√£o as fontes externas que escrevem dados na solu√ß√£o:

- **Mobile App**: utilizado pelo motorista (envia e consome dados)
- **Backoffice**: utilizado por supervisores (envia e consome dados)
- **Other**: integra√ß√µes ou scripts (apenas enviam dados)

Todas as fontes se comunicam com a API hospedada em [**Cloud Run**](https://cloud.google.com/run), via chamadas HTTP seguindo o padr√£o REST, utilizando o m√©todo adequado para cada tipo de opera√ß√£o:

| M√©todo   | Objetivo                 | Exemplo               |
| -------- | ------------------------ | --------------------- |
| `POST`   | Inser√ß√£o de registros    | `POST /motoristas`    |
| `PUT`    | Atualiza√ß√£o de registros | `PUT /viagens/123`    |
| `DELETE` | Remo√ß√£o de registros     | `DELETE /clientes/12` |

```http
PUT /viagens/123
Content-Type: application/json

{
  "status": "conclu√≠da",
  "data_fim": "2025-04-03T18:00:00Z"
}
```

### Metadados

Cada requisi√ß√£o carrega metadados, que ser√£o usados nas etapas seguintes:

| Metadado    | Origem          | Utilidade                                              |
| ----------- | --------------- | ------------------------------------------------------ |
| `timestamp` | gerado pela API | Para ordena√ß√£o e auditoria                             |
| `auditid`   | gerado pela API | Id √∫nico da requisi√ß√£o, usado no ciclo de vida do dado |
| `source`    | header ou token | Identifica a origem (ex: app, backoffice)              |
| `operation` | m√©todo HTTP     | Define se √© insert/update/delete                       |
| `entity`    | path da URL     | Define a tabela/entidade                               |
| `userid`    | path da URL     | Identifica quem gerou o registro                       |

Ao receber uma requisi√ß√£o de qualquer uma das fontes externas, a API em **Cloud Run** realiza duas a√ß√µes:

- **Log da requisi√ß√£o**: o conte√∫do recebido, os headers, o status HTTP e o tempo de resposta s√£o registrados automaticamente no [**Cloud Logging**](https://cloud.google.com/logging), garantindo rastreabilidade.

- **Encaminhamento dos dados**: ap√≥s o processamento inicial, o dado √© bifurcado, persistindo em um banco relacional (utilizado para leituras r√°pidas) e publicando no [**Pub/Sub**](https://cloud.google.com/pubsub) (onde ser√° processado de forma ass√≠ncrona).

> Os logs gerados podem ser analisados via [**Log Analytics**](https://cloud.google.com/logging/docs/log-analytics), possibilitando a cria√ß√£o de alertas e dashboards que ajudam a identificar comportamentos inesperados.



## 2. Persist√™ncia no PostgreSQL üíæ

O dado √© processado de acordo com a regra de neg√≥cio e salvo no banco relacional PostgreSQL em inst√¢ncia prim√°ria, fazendo uso do [**Cloud SQL**](https://cloud.google.com/sql).

Consultas operacionais e anal√≠ticas s√£o realizadas por meio de uma inst√¢ncia de r√©plica, que serve de apoio para relat√≥rios que exigem menor lat√™ncia, sem sobrecarregar a inst√¢ncia prim√°ria.

Caso ocorra algum erro na escrita, a transa√ß√£o √© revertida e a resposta HTTP reflete a falha.



## 3. Publica√ß√£o no Pub/Sub üì¨

Ap√≥s o recebimento da requisi√ß√£o, a API publica um evento no t√≥pico do [**Pub/Sub**](https://cloud.google.com/pubsub) e a mensagem √© entregue ao pipeline do **Dataflow**. O **Pub/Sub** garante a entrega da mensagem por meio de tentativas autom√°ticas:
- Cada mensagem pode ser reenviada at√© m√°ximo de tentativas (ex: 5 vezes)
- O consumidor deve confirmar o recebimento com um `ack`
- Se falhar ap√≥s as tentativas, a mensagem √© enviada para o [**DLQ**](https://cloud.google.com/pubsub/docs/handling-failures)

### Mensagens na DLQ

A **DLQ** √© implementada como um segundo t√≥pico **Pub/Sub**, recebendo mensagens que excederam o m√°ximo de tentativas para subscription principal:
- O t√≥pico da **DLQ** √© integrado ao **BigQuery**, utilizando a [funcionalidade nativa de ingest√£o](https://cloud.google.com/pubsub/docs/bigquery) via **Pub/Sub**.
- Toda mensagem publicada na **DLQ** √© inserida em uma tabela, sem necessidade de pipeline adicional.

```json
{
  "timestamp": "2025-04-03T19:15:32.123Z",
  "auditid": "req-123abc",
  "userid": "abc321",
  "source": "backoffice",
  "operation": "update",
  "entity": "viagens",
  "id": "abc123",
  "data": {
    "id": "abc123",
    "status": "conclu√≠da",
    "data_fim": "2025-04-03T18:00:00Z"
  },
  "messageid": "abc123",
  "publishtime": "2025-04-03T19:15:33.543Z"
}
```



## 4. Processamento com Dataflow üö¥‚Äç‚ôÇÔ∏è

O pipeline do [**Dataflow**](https://cloud.google.com/products/dataflow) √© respons√°vel por consumir os eventos publicados no t√≥pico do Pub/Sub e grav√°-los no BigQuery da forma mais simples poss√≠vel.

Como o fluxo foi projetado para operar em **modo batch**, o Dataflow agrupa eventos em janelas de tempo configur√°veis (ex: a cada 5 minutos), garantindo redu√ß√£o de custos:
- Leitura direta do evento JSON recebido do **Pub/Sub**
- Separa√ß√£o e roteamento dos eventos de acordo com a entidade (ex: viagens, motoristas, clientes)
- Persist√™ncia em tabelas separadas dentro do dataset `raw`, uma para cada entidade
- Encaminhamento de falhas internas do **Dataflow** (ex: parsing inv√°lido) para tabelas de erro espec√≠ficas no **BigQuery**

> Desenho com suporte a esquemas flex√≠veis, utilizando o tipo `data:STRING` no BigQuery. Isso permite armazenar os dados recebidos independentemente de mudan√ßas na estrutura do payload.



## 5. Armazenamento no BigQuery üóÉÔ∏è

### Dataset `raw`

No [**BigQuery**](https://cloud.google.com/bigquery), os eventos processados pelo **Dataflow** s√£o armazenados em um dataset chamado `raw`. Neste dataset, cada entidade possui sua pr√≥pria tabela. Essa abordagem traz maior organiza√ß√£o, performance e facilidade de evolu√ß√£o do schema de forma independente.

- `raw.viagens`
- `raw.motoristas`
- `raw.clientes`
- `raw_errors.viagens` (para eventos malformados)

#### Estrutura esperada para tabelas no `raw`

| Coluna              | Tipo        | Descri√ß√£o                                          |
| ------------------- | ----------- | -------------------------------------------------- |
| `timestamp`         | TIMESTAMP   | Momento em que o dado foi gerado                   |
| `auditid`           | STRING      | ID √∫nico da requisi√ß√£o                             |
| `userid`            | STRING      | ID √∫nico do usu√°rio                                |
| `operation`         | STRING      | Tipo de opera√ß√£o: insert, update ou delete         |
| `source`            | STRING      | Origem da requisi√ß√£o (app, backoffice etc)         |
| `id`                | STRING      | Identificador √∫nico do registro                    |
| `data`              | STRING      | Payload original enviado                           |
| `messageid`         | STRING      | ID √∫nico da mensagem no Pub/Sub                    |
| `publishtime`       | TIMESTAMP   | Momento em que a mensagem foi publicada no Pub/Sub |
| `ingestiondatetime` | TIMESTAMP   | Momento em que o dado foi gravado no BigQuery      |

> As tabelas s√£o particionadas por data, no caso `DATE(timestamp)` e podem ser clusterizadas por `id` e `operation` para melhorar o desempenho nas consultas.
>
> Eventos inv√°lidos ou malformados s√£o armazenados em tabelas espelhadas no dataset `raw_errors`, com a mesma estrutura da `raw`.



## 6. Transforma√ß√µes com Dataform üßû‚Äç‚ôÇÔ∏è

O [**Dataform**](https://cloud.google.com/dataform) √© respons√°vel por organizar os dados armazenados nas tabelas do dataset `raw`, aplicando regras de neg√≥cio, valida√ß√µes adicionais e modelagens necess√°rias para disponibilizar os dados prontos para an√°lise.

### Staging

Nesta etapa, cada tabela do dataset `raw` possui sua respectiva tabela no dataset `staging`. Essas tabelas s√£o atualizadas com base em transforma√ß√µes agendadas, geralmente via DAGs ou jobs do pr√≥prio Dataform.

As tabelas realizam:
- Leitura dos dados brutos (`raw.*`)
- Extra√ß√£o de campos do JSON `data`
- Padroniza√ß√£o de nomes e formatos
- Remo√ß√£o de duplicidades
- Filtro de registros inv√°lidos ou desnecess√°rios

#### `staging.viagens`

| Coluna     | Tipo      | Descri√ß√£o                         |
| ---------- | --------- | --------------------------------- |
| `id`       | STRING    | Identificador √∫nico da viagem     |
| `status`   | STRING    | Status atual da viagem            |
| `data_fim` | TIMESTAMP | Data e hora de conclus√£o da viagem |
| `lasttimestamp` | TIMESTAMP | Data e hora da √∫ltima request que gerou o registro |
| `lastuserid` | STRING | √öltimo usu√°rio que gerou o registro |
| `lastoperation` | TIMESTAMP | √öltima opera√ß√£o realizada no registro |
| `ingestiondatetime` | TIMESTAMP | Momento em que o dado foi gravado no BigQuery |

```sql
config {
  type: "table",
  partition_by: "DATE(ingestiondatetime)",
  schema: "staging",
  name: "viagens"
}

SELECT
  id,
  MAX_BY(data.status, timestamp) AS status,
  MAX_BY(data.data_fim, timestamp) AS data_fim,
  MAX(timestamp) AS lasttimestamp,
  MAX_BY(userid, timestamp) AS lastuserid,
  MAX_BY(operation, timestamp) AS lastoperation,
  CURRENT_TIMESTAMP() AS ingestiondatetime
FROM raw.viagens
WHERE operation != 'delete'
GROUP BY id
```

> Esta tabela √© particionada por `DATE(ingestiondatetime)`, o que permite melhorar a performance de leitura e reduzir os custos nas consultas anal√≠ticas.
>
> Como padr√£o, utilizamos `MAX_BY(campo, timestamp)` para reconstruir o estado completo do registro quando os eventos s√£o parciais. Isso garante que cada campo traga o valor mais recente, mesmo que as atualiza√ß√µes venham em diferentes momentos e com apenas parte dos dados.

### Gold

Representa dados prontos para consumo por ferramentas de BI, como o **Looker Studio**. Os modelos s√£o constru√≠dos a partir das tabelas da `staging`, geralmente por meio de jobs agendados no pr√≥prio **Dataform**. Essa camada costuma conter dados j√° enriquecidos, validados e organizados de acordo com a necessidade de visualiza√ß√µes e relat√≥rios de neg√≥cio.

#### `gold.problemas_motoristas_24h`

| Coluna                  | Tipo      | Descri√ß√£o                                        |
|-------------------------|-----------|--------------------------------------------------|
| `motorista_nome`        | STRING    | Nome completo do motorista                       |
| `motorista_telefone`    | STRING    | Telefone de contato do motorista                 |
| `problema_nome`         | STRING    | Tipo de problema identificado                    |
| `problema_oficina`      | STRING    | Nome da oficina associada ao problema            |
| `problema_oficina_contato` | STRING | Contato da oficina para encaminhamento           |
| `problema_horario`      | TIMESTAMP | Hor√°rio em que o problema foi registrado         |

```sql
config {
  type: "table",
  schema: "gold",
  name: "problemas_motoristas_24h",
  partition_by: "DATE(horario)"
}

WITH problemas_motoristas AS (
  SELECT
    id_problema,
    id_motorista,
    horario
  FROM staging.problemas_motoristas
  WHERE horario >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 24 HOUR)
)

SELECT
  m.nome AS motorista_nome,
  m.telefone AS motorista_telefone,
  p.nome AS problema_nome,
  p.oficina AS problema_oficina,
  p.oficina_contato AS problema_oficina_contato,
  pm.horario AS problema_horario
FROM problemas_motoristas AS pm
LEFT JOIN staging.motoristas AS m ON pm.id_motorista = m.id
LEFT JOIN staging.problemas AS p ON pm.id_problema = p.id
```
> Tabela re-gerada uma vez ao dia, mas poderiamos trabalhar com casos de carga incremental afim de ter o hist√≥rico dos dados (estrat√©gia de carga a partir do max counter da √∫ltima ingest√£o).



## 7. Consumo dos dados üç´

Ap√≥s as transforma√ß√µes e modelagens nas camadas `staging` e `gold`, os dados ficam dispon√≠veis para consumo por diferentes canais, de acordo com a necessidade da aplica√ß√£o ou do time de neg√≥cio.

### Looker Studio

O [**Looker Studio**](https://lookerstudio.google.com/) pode se conectar tanto ao **BigQuery** quanto ao **PostgreSQL** (r√©plica). A escolha da fonte depende do tipo de dado e da necessidade de lat√™ncia:
- Quando conectado ao **BigQuery**, pode-se fazer uso de mecanismos de cache para evitar a cria√ß√£o de novos jobs a cada acesso e diminuir o tempo de resposta.
- Quando conectado ao **PostgreSQL** (r√©plica), o **Looker Studio** permite consultas com menor lat√™ncia, ideais para dados operacionais ou dashboards em tempo quase real.

### Processos anal√≠ticos e modelos de IA

Os dados da camada `staging` e `gold` tamb√©m podem ser utilizados como insumo para:
- Treinamento de modelos de machine learning
- Execu√ß√£o de rotinas batch anal√≠ticas
- Previs√µes operacionais



## Notas ‚úçÔ∏è

### Custos

- Embora n√£o seja uma ferramenta de f√°cil uso, a Google disponibiliza uma [calculadora](https://cloud.google.com/products/calculator) para auxiliar com rela√ß√£o aos custos. Tamb√©m √© poss√≠vel mensurar consultando a documenta√ß√£o de cada um dos recursos utilizados.

### Sofistica√ß√£o

- √â poss√≠vel criar uma **Cloud Run** e conecta-la ao **BigQuery** para chama-la durante a execu√ß√£o de uma query ([mais detalhes](https://cloud.google.com/bigquery/docs/connections-api-intro)), possibilitando por exemplo, informar os motoristas por meio de alertas.

- √â poss√≠vel criar uma conex√£o federada no **BigQuery** para que ele acesse o banco relacional ([mais detalhes](https://cloud.google.com/bigquery/docs/federated-queries-intro)), possibilitando enriquecer os relat√≥rios.

### Adapta√ß√µes

- Na etapa 2, caso n√£o haja uma frequ√™ncia de envio de dados muito alta, o uso da replica no Cloud SQL pode ser deixado de lado.

- As etapas 3 e 4 podem ser removidas, sendo poss√≠vel enviar dados por meio do [**BigQuery Write API**](https://cloud.google.com/bigquery/docs/write-api) pela **Cloud Run**.

- A etapa 4 pode ser removida, enviando dados diretamente do **Pub/Sub** para o **BigQuery** ([mais detalhes](https://cloud.google.com/pubsub/docs/bigquery)).

- Caso o **Dataform** seja uma op√ß√£o muito robusta, ele pode ser substituido por [**Scheduled Queries**](https://cloud.google.com/bigquery/docs/scheduling-queries).

### Melhorias

- Arquitetura proposta com fins did√°ticos, pass√≠vel de customiza√ß√µes conforme o contexto e o volume de dados de cada aplica√ß√£o.

---

<p align="center">Made with ‚ù§Ô∏è in Curitiba üå≥ ‚òîÔ∏è</p>
